
# Introduction

Data are a fundamental currency upon which scientific discoveries are made. Without access to good data, it becomes extremely difficult if not impossible to advance science. Yet, a large majority of data on which published research papers are based rarely see the light of day and are only ever made available to the original authors (TODO: Citation).  Sharing datasets upon publication of a research paper has benefits to individual researchers, often through increased visibility of the work. But there is also a component of benefit to the broader scientific community. This primarily comes in the form of potential for reuse in other contexts, and use for training and teaching (@McKiernan2016). Assuming that the data have no privacy concerns (e.g. patient data, locations of critically endangered species), or that the act of sharing doesn't put the authors at a competitive disadvantage, sharing data will always have a net positive benefit. First and foremost, sharing data along with other artifacts can make it easier for a reader to independently reproduce the results, thereby increasing transparency and trust in the work. The act of easy data sharing can also improves model training, as many different models can be tried and tested on latest data sources, closing the loop on research and application of statistical techniques. Existing datasets can be combined or linked with new or existing data, fostering the development and synthesis of new ideas and research areas. The biggest of these benefits is the overall increase in reproducibility.

For nearly two decades, researchers who work in areas related to computational science have pushed for better standards to verify scientific claims, especially in areas where a full replication of the study would be prohibitively expensive. To meet these minimal standards, there must be easy access to the data, models, and code. Among the different areas with a computational bent, the bioinformatics community in particular has a strong culture around open source (cite https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-10-r80), and has made releasing code and associated software a recognized mainstream activity. Many journals in these fields have also pushed authors to submit code (model implementations) alongside their papers, with a few journals going as far as providing a "reproducibility review" (cite http://science.sciencemag.org/content/334/6060/1226/tab-pdf).

For a piece of computational research to be reproducible, it requires three distinct elements:
1. Code
2. Computing environment
3. Data

The first two of these challenges have largely been solved. 

The past decade has seen the rapid rise of Data Science as a discipline distinct from statistics [@Donoho2017]. This rapid growth in data science has largely been fueled by easy access to open source software tools. Programming languages such as Python, R and Julia and help scientists implement and develop new methods to work with data. Each of these languages is supported by thriving communities of researchers and software developers who contribute many of the building blocks that make them popular. As of this writing, Python, Julia, and R have  167k packages (https://pypi.org/), ~ 14k packages (https://cran.r-project.org/) and ~2k  packages (https://pkg.julialang.org/) respectively. These packages form the building blocks of a data scientists daily workflow. 

A typical data scientist loads a dozen or two of these libraries at the top of a notebook and rely on existing routines to rapidly read, transform, visualize, and model data. Each of these packages individually depend on a complex web of other packages, building upon existing work rather than reimplementing everything from scratch. Working from script and a list of such dependencies, a data scientist can easily install all the necessary tools in any local or remote environment and reproduce the computation. When new functionality is developed, it is packaged into a separate entity and added to a languages package manager.

The computing environment is easily captured with modern tools such as Docker [@docker; @binder]. Modern tools such as Binder (citation) can parse Docker files and dependency trees to provide on demand, live notebooks in R and Python that a reader can immediately execute in the browser without any local installation. This makes it simple to load a specific environment to run any analysis. Code is handled by version control with tools like Git and GitHub [@git; @github], paired with archiving such as Zenodo and figshare provide access to code (particularly model implementations)[@zenodo; @figshare]. All the necessary software is available from various package managers (and their numerous geographic mirrors and archives) making it easy to install any version of a software package. However, the biggest challenge, even today, remains easy and repeatable access to data in a data science notebook.

Although there are numerous public and private data repositories, none  of them function as package managers. Datasets are also far more diverse in complexity, size, and formats, making them particularly challenging to standardize. As a result, data used in an analysis may be read from various locations (local, network, or cloud), various formats, varying levels of tidyness (cite tidy data). 

What aspects of data make them particularly challenging to share from a reproducibility perspective? This is the question we tackle in this paper.  While there are several papers that serve as best-practice guides for formatting data and getting them ready for sharing, the aims of this paper are a bit different. Our aim to address the issue of data in the context of reproducibility in data science workflows. In particular we discuss the various tradeoffs one has to consider when documenting and sharing data, and how this would change depending on the use case, perceived impact, and potential audience.

 We discuss how to publish data, and detail the technical issues related to this, as well as challenges on licensing, and data embargoes (especially for sensitive data). We also analyze the state of data, particularly how it is packaged, shared and made available as part of modern data analysis. How often are data shipped inside software packages? How does data format and size impact availability? 
